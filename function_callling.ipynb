{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://www.youtube.com/watch?v=Ss_GdU0KqE0&t=182s\n",
    "# comparativos entre modelos que aceitam funções: https://gorilla.cs.berkeley.edu/blogs/4_open_functions.html\n",
    "# https://gorilla.cs.berkeley.edu/leaderboard.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\"The Future of Freight: How Large Language Models are Revolutionizing Logistics\"**\n",
      "\n",
      "As a shipping magnate, I've spent my career navigating the complexities of global trade. From optimizing routes to managing supply chains, every decision has been driven by a singular goal: getting goods from point A to point B as efficiently and effectively as possible.\n",
      "\n",
      "But in recent years, a new player has entered the scene that's changing the game for logistics professionals like myself: Large Language Models (LLMs). These AI-powered tools are capable of processing vast amounts of data, generating insights, and even making decisions – all with unprecedented speed and accuracy.\n",
      "\n",
      "At first glance, it might seem counterintuitive to think that LLMs would have a significant impact on the shipping industry. After all, aren't they just fancy language generators? But trust me, these models are far more than that. They're game-changers.\n",
      "\n",
      "One of the most significant advantages of LLMs is their ability to analyze and interpret vast amounts of data in real-time. In logistics, this means being able to track shipments, monitor weather patterns, and anticipate potential disruptions – all with a level of precision that was previously unimaginable.\n",
      "\n",
      "For example, our company recently implemented an LLM-powered system to optimize our route planning. By analyzing historical traffic patterns, road conditions, and other factors, the model is able to suggest the most efficient routes for our drivers – saving us time, fuel, and reducing emissions in the process.\n",
      "\n",
      "But that's just the tip of the iceberg. LLMs are also being used to improve communication between shippers, carriers, and customers. By generating clear, concise language that's tailored to each individual's needs, these models can help reduce misunderstandings and errors – a major source of frustration for logistics professionals like myself.\n",
      "\n",
      "And then there's the issue of scalability. As global trade continues to grow at an unprecedented rate, the need for efficient logistics solutions has never been greater. LLMs are perfectly positioned to meet this challenge, as they're capable of processing vast amounts of data and generating insights that can inform decision-making at scale.\n",
      "\n",
      "Of course, there are also concerns about the potential impact of LLMs on jobs in the shipping industry. As with any new technology, there's a risk that automation could displace certain roles – but I firmly believe that this is a net positive for our industry as a whole.\n",
      "\n",
      "By freeing up human resources from mundane tasks like data entry and route planning, we can focus on higher-level decision-making and strategic planning – areas where creativity, empathy, and problem-solving skills are essential. And let's be honest: who wouldn't want to spend their days analyzing complex logistics scenarios and finding innovative solutions?\n",
      "\n",
      "As a shipping magnate, I'm excited to see the impact that LLMs will have on our industry in the years to come. From optimizing routes to improving communication, these models are poised to revolutionize the way we do business – and I couldn't be more thrilled.\n",
      "\n",
      "So if you're a logistics professional looking for ways to stay ahead of the curve, take note: Large Language Models are here to stay – and they're about to change the game."
     ]
    }
   ],
   "source": [
    "# testing Ollama - https://github.com/samwit/agent_tutorials/blob/main/ollama_agents/llama3_local/testing_ollama.py\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Local Llama3 \n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1\",\n",
    "    temperature=0,\n",
    "    max_new_tokens=512)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Write me a 500 word article on {topic} from the perspective of a {profession}. \")\n",
    "\n",
    "# using LangChain Expressive Language chain syntax\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "\n",
    "# print(chain.invoke({\"topic\": \"LLMs\", \"profession\": \"shipping magnate\"}))\n",
    "\n",
    "for chunk in chain.stream({\"topic\": \"LLMs\", \"profession\": \"shipping magnate\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'properties': {'name': {'type': 'string'}, 'age': {'type': 'integer'}, 'hobbies': {'type': 'array', 'items': {'type': 'string'}}}}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "json_schema = {\n",
    "    \"title\": \"Person\",\n",
    "    \"description\": \"Identifying information about a person.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"name\": {\"title\": \"Name\", \"description\": \"The person's name\", \"type\": \"string\"},\n",
    "        \"age\": {\"title\": \"Age\", \"description\": \"The person's age\", \"type\": \"integer\"},\n",
    "        \"favorite_food\": {\n",
    "            \"title\": \"Fav Food\",\n",
    "            \"description\": \"The person's favorite food\",\n",
    "            \"type\": \"string\",\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"name\", \"age\",\"fav_food\"],\n",
    "}\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1\",\n",
    "    format=\"json\",\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=512\n",
    "    )\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(\n",
    "        content=\"Please tell me about a person using the following JSON schema:\"\n",
    "    ),\n",
    "    HumanMessage(content=\"{schema}\"),\n",
    "    HumanMessage(\n",
    "        content=\"Now, just considering the schema, tell me about a person named John who is 35 years old and loves pizza.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "#converting the json schema to a string\n",
    "dumps = json.dumps(json_schema, indent=2)\n",
    "\n",
    "# chain = prompt | llm | StrOutputParser()\n",
    "chain = prompt | llm | JsonOutputParser()\n",
    "\n",
    "response = chain.invoke({\"schema\": dumps})\n",
    "print(response)\n",
    "print(type(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"John\",\n",
      "  \"age\": 35,\n",
      "  \"hobbies\": [\n",
      "    \"pizza\"\n",
      "  ]\n",
      "}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# json schema\n",
    "# format=\"json\"\n",
    "\n",
    "\n",
    "import json\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "json_schema = {\n",
    "    \"title\": \"Person\",\n",
    "    \"description\": \"Identifying information about a person.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"name\": {\"title\": \"Name\", \"description\": \"The person's name\", \"type\": \"string\"},\n",
    "        \"age\": {\"title\": \"Age\", \"description\": \"The person's age\", \"type\": \"integer\"},\n",
    "        \"favorite_food\": {\n",
    "            \"title\": \"Favorite Food\",\n",
    "            \"description\": \"The person's favorite food\",\n",
    "            \"type\": \"string\",\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"name\", \"age\", \"favorite_food\"],\n",
    "}\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1\",\n",
    "    format=\"json\",\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(content=\"Please provide information about a person using the following JSON schema:\"),\n",
    "    HumanMessage(content=\"{schema}\"),\n",
    "    HumanMessage(content=\"Generate information for a person named John who is 35 years old and loves pizza.\"),\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "# Convert the JSON schema to a string\n",
    "schema_str = json.dumps(json_schema, indent=2)\n",
    "\n",
    "chain = prompt | llm | JsonOutputParser()\n",
    "\n",
    "response = chain.invoke({\"schema\": schema_str})\n",
    "print(json.dumps(response, indent=2))\n",
    "print(type(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Alex' height=5.0 hair_color='blonde'\n"
     ]
    }
   ],
   "source": [
    "# Ollama functions\n",
    "# pydantic\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "#from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "# Pydantic Schema for structured response\n",
    "class Person(BaseModel):\n",
    "    name: str = Field(description=\"The person's name\", required=True)\n",
    "    height: float = Field(description=\"The person's height\", required=True)\n",
    "    hair_color: str = Field(description=\"The person's hair color\")\n",
    "\n",
    "context = \"\"\"Alex is 5 feet tall. \n",
    "Claudia is 1 feet taller than Alex and jumps higher than him. \n",
    "Claudia is a brunette and Alex is blonde.\"\"\"\n",
    "\n",
    "# Prompt template llama3\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a smart assistant take the following context and question below and return your answer in JSON.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "QUESTION: {question} \\n\n",
    "CONTEXT: {context} \\n\n",
    "JSON:\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    " \"\"\"\n",
    ")\n",
    "\n",
    "# Chain\n",
    "llm = ChatOllama(model=\"llama3.1\",format=\"json\", temperature=0)\n",
    "\n",
    "structured_llm = llm.with_structured_output(Person)\n",
    "chain = prompt | structured_llm\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"question\": \"Who is taller?\",\n",
    "    \"context\": context\n",
    "    })\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' response_metadata={'model': 'llama3.1', 'created_at': '2024-09-05T19:34:13.256776959Z', 'message': {'role': 'assistant', 'content': '', 'tool_calls': [{'function': {'name': 'get_current_weather', 'arguments': {'location': 'Singapore', 'unit': 'fahrenheit'}}}]}, 'done_reason': 'stop', 'done': True, 'total_duration': 8453431741, 'load_duration': 56296185, 'prompt_eval_count': 182, 'prompt_eval_duration': 4884333000, 'eval_count': 25, 'eval_duration': 3462767000} id='run-25b07880-9a6f-4de7-9bfb-889f78c2d564-0' tool_calls=[{'name': 'get_current_weather', 'args': {'location': 'Singapore', 'unit': 'fahrenheit'}, 'id': '596f3d8b-5806-4af0-b61d-ba69ba684d6d', 'type': 'tool_call'}] usage_metadata={'input_tokens': 182, 'output_tokens': 25, 'total_tokens': 207}\n"
     ]
    }
   ],
   "source": [
    "# bind tools\n",
    "\n",
    "model = ChatOllama(model=\"llama3.1\", temperature=0)\n",
    "\n",
    "model = model.bind_tools(\n",
    "    tools=[\n",
    "        {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, \" \"e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"unit\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    "    function_call={\"name\": \"get_current_weather\"},\n",
    ")\n",
    "\n",
    "response = model.invoke(\"what is the weather in Singapore?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "https://python.langchain.com/v0.2/docs/how_to/tool_calling/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repare que o LLM não executa uma função, ele apenas indica que uma função deve ser executada e os parâmetros que devem ser passados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function name, type hints, and docstring are all part of the tool\n",
    "# schema that's passed to the model. Defining good, descriptive schemas\n",
    "# is an extension of prompt engineering and is an important part of\n",
    "# getting models to perform well.\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "tools = [add, multiply]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', response_metadata={'model': 'llama3.1', 'created_at': '2024-09-05T19:34:23.765663701Z', 'message': {'role': 'assistant', 'content': '', 'tool_calls': [{'function': {'name': 'multiply', 'arguments': {'a': 3, 'b': 12}}}]}, 'done_reason': 'stop', 'done': True, 'total_duration': 10388939387, 'load_duration': 58526876, 'prompt_eval_count': 216, 'prompt_eval_duration': 7232744000, 'eval_count': 22, 'eval_duration': 3055357000}, id='run-014fef60-77d2-4fe8-9c94-62d861183111-0', tool_calls=[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': '252292cd-f66f-4452-b3ac-1708683bfc1b', 'type': 'tool_call'}], usage_metadata={'input_tokens': 216, 'output_tokens': 22, 'total_tokens': 238})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1\", temperature=0)\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "query = \"What is 3 * 12?\"\n",
    "llm_with_tools.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'multiply',\n",
       "  'args': {'a': 3, 'b': 12},\n",
       "  'id': '194fb237-86ac-4185-90e8-5b474149c590',\n",
       "  'type': 'tool_call'},\n",
       " {'name': 'add',\n",
       "  'args': {'a': 11, 'b': 49},\n",
       "  'id': 'bb10b46f-25d5-4ada-bb82-f28600af1c67',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is 3 * 12? Also, what is 11 + 49?\"\n",
    "\n",
    "llm_with_tools.invoke(query).tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[multiply(a=3, b=12), add(a=11, b=49)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parsing\n",
    "from langchain_core.output_parsers import PydanticToolsParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class add(BaseModel):\n",
    "    \"\"\"Add two integers.\"\"\"\n",
    "\n",
    "    a: int = Field(..., description=\"First integer\")\n",
    "    b: int = Field(..., description=\"Second integer\")\n",
    "\n",
    "\n",
    "class multiply(BaseModel):\n",
    "    \"\"\"Multiply two integers.\"\"\"\n",
    "\n",
    "    a: int = Field(..., description=\"First integer\")\n",
    "    b: int = Field(..., description=\"Second integer\")\n",
    "\n",
    "\n",
    "chain = llm_with_tools | PydanticToolsParser(tools=[add, multiply])\n",
    "chain.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "- https://python.langchain.com/v0.2/docs/how_to/tool_results_pass_to_model/\n",
    "- https://python.langchain.com/v0.2/docs/how_to/custom_tools/#creating-tools-from-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies a and b.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "tools = [add, multiply]\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': '5d04c0fc-cc90-4f9a-a924-bbbbee451a43', 'type': 'tool_call'}, {'name': 'add', 'args': {'a': 11, 'b': 60}, 'id': 'f8057ba1-efad-4e27-a598-a62d811aea43', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "query = \"What is 3 * 12? Also, what is 11 + 49?\"\n",
    "\n",
    "messages = [HumanMessage(query)]\n",
    "\n",
    "ai_msg = llm_with_tools.invoke(messages)\n",
    "\n",
    "print(ai_msg.tool_callsb)\n",
    "\n",
    "messages.append(ai_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiply\n",
      "{'a': 3, 'b': 12}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tools_calls = ai_msg.tool_calls\n",
    "\n",
    "# Parse tool name and arguments\n",
    "tool_name = tools_calls[0]['name']\n",
    "arguments = tools_calls[0]['args']\n",
    "#city = arguments['city']\n",
    "print(tool_name)\n",
    "print(arguments)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is 3 * 12? Also, what is 11 + 49?'),\n",
       " AIMessage(content='', response_metadata={'model': 'llama3.1', 'created_at': '2024-09-05T19:35:17.032481013Z', 'message': {'role': 'assistant', 'content': '', 'tool_calls': [{'function': {'name': 'multiply', 'arguments': {'a': 3, 'b': 12}}}, {'function': {'name': 'add', 'arguments': {'a': 11, 'b': 60}}}]}, 'done_reason': 'stop', 'done': True, 'total_duration': 22906487983, 'load_duration': 59756470, 'prompt_eval_count': 221, 'prompt_eval_duration': 7058527000, 'eval_count': 107, 'eval_duration': 15786267000}, id='run-38cb0390-52cd-47ea-9601-fc232c9dcc7b-0', tool_calls=[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': '5d04c0fc-cc90-4f9a-a924-bbbbee451a43', 'type': 'tool_call'}, {'name': 'add', 'args': {'a': 11, 'b': 60}, 'id': 'f8057ba1-efad-4e27-a598-a62d811aea43', 'type': 'tool_call'}], usage_metadata={'input_tokens': 221, 'output_tokens': 107, 'total_tokens': 328}),\n",
       " ToolMessage(content='36', name='multiply', tool_call_id='5d04c0fc-cc90-4f9a-a924-bbbbee451a43'),\n",
       " ToolMessage(content='71', name='add', tool_call_id='f8057ba1-efad-4e27-a598-a62d811aea43')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for tool_call in ai_msg.tool_calls:\n",
    "    selected_tool = {\"add\": add, \"multiply\": multiply}[tool_call[\"name\"].lower()]\n",
    "    tool_msg = selected_tool.invoke(tool_call)\n",
    "    messages.append(tool_msg)\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Based on the tool call responses, I can provide the following answers:\\n\\n* 3 * 12 = 36\\n* 11 + 49 = 60', response_metadata={'model': 'llama3.1', 'created_at': '2024-09-05T19:35:26.913373Z', 'message': {'role': 'assistant', 'content': 'Based on the tool call responses, I can provide the following answers:\\n\\n* 3 * 12 = 36\\n* 11 + 49 = 60'}, 'done_reason': 'stop', 'done': True, 'total_duration': 9858929835, 'load_duration': 24358526, 'prompt_eval_count': 113, 'prompt_eval_duration': 4392205000, 'eval_count': 34, 'eval_duration': 5223377000}, id='run-bd1c1a72-9699-4ea5-9e26-5667f564b184-0', usage_metadata={'input_tokens': 113, 'output_tokens': 34, 'total_tokens': 147})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the tool call responses, I can provide the following answers:\\n\\n* 3 * 12 = 36\\n* 11 + 49 = 60'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools.invoke(messages).content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tool calling agents, like those in LangGraph, use this basic flow to answer queries and solve tasks.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
